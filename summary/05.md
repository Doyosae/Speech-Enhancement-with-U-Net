# Deep Speech Enhancement for Reverberated and Noisy Signals using Wide Residual Networks 
## Abstract
- 이 논문은 wide neural network 아키텍처에서 residual 연결의 높은 잠재성으로 특징을 추출하는 deep speech enhancement method를 제안한다. 
- Wdie Residual Network는 시간 도메인에서 계산하는 1차원 컨볼루션 신경망을 포함하며, Speech feature seqeunce와 같은 temporal domain에서 맥락상 상관된 표현들을 처리하는 매우 강력한 방법이다.
- 신호는 항상 선형적으로 다루어지고, 비선형적 경로는 몇몇 단계에서 adding, subtracting 되어 향상시킬수 있다. 이러한 면에서 Residual 메커니즘은 enhancement taks에 매우 유용하다.
- 제안한 방법에서 Speech Enhancement는 intelligibility purposes and the speech recognition system에서 모두 성공적이다.
- 이 심층 신경망 모델은 reveberation data 뿐만 아니라, 전통적인 개선 방법에서 어려운 문제인 low noise level에서도 장점이 있다.
#
## Introdunction
- 이 논문은 Wide Residual Neural Networks라는 최신의 심층 신경망 구조을 스피치 처리에 적용하는 것을 제안한다.
- 줄여서 WRN, 이 구조는 실제 세계의 복잡한 음향 왜곡을 잘 근사하는 estimator를 설계한다.
- 그래서 우리가 제안하는 프레임워크의 성능이 전통적인 통계 기반 접근 방식을 훨씬 능가한다는 것을 보여줄 것이다.
- 이 논문의 순서는 Speech Enhancement에 사용하는 몇 가지 심층 신경망 솔루션을 이야기하고, 그 다음에 WRN에 대해 자세한 이야기를 한다.
그리고 마지막에는 수행한 실험 설정에 대해 이야기하고, 획득한 결과에 대하여 논의한다. 
#
## Deep Speech Enhancement
- 딥 러닝을 아울러 데이터 기반 패러다임은 노이즈 데이터와 깨끗한 데이터의 상관관계를 모델링하는데 매우 적합하다.
- 이러한 컨셉이 비록 컴퓨터 비전 분야에서 먼저 적용되었을지라도, Speech enhancement에 또한 적합하다.
- DAE를 넘어서, Speech Enhancement 문제는 앞서 Computer VIsion 분야에서 성공적으로 증명된 딥 러닝 모델 구조들을 잘 적용할 수 있다.
### RNN
- LSTM을 필두로한 RNN 신경망은 음성 인식에서 매우 훌륭한 성능을 보인다. 재귀적 연결이 Temporal한 데이터의 상관관계를 잘 포착한다.
- 그러나 제한된 노이즈 타입으로 학습하다면 RNN은 그것의 일반화 측면에서 성능이 낮다.
### CNN
- CNN 구조는 frequency와 temporal 도메인에서, 즉 spectrogram에서 지역화된 패턴을 추출하는데 유리하다.
- Reverberation의 모양은 특정 time freqeuncy area에서 신호 스펙트럼이 확장된 모습으로 나타난다. 
- Consecutive한 TF가 자연스럽거나 왜곡된 신호의 구조 속에서 맥락 관계가 있다는 것을 보여준다.
- CNN은 이러한 신호 특징들을 다루는 것에 효과적이다. 그러므로 speech enhancement에 강력하다.
- 그 중 Residual Network와 데이터들의 dynamic correlation, consecutive frames을 결합한 것은 좋은 생각이다.
- 왜냐하면 residual network의 손상된 신호의 기본 구조에 대하여 풍부하고 섬세한 표현력을 보여줄 수 있기 때문이다.
- 이전까지 residual network는 speech enhancement에서 거의 쓰이지 않았다.
### Adversarial Networks
- GAN에 대한 이야기는 따로 그것에 대한 논문을 보기로 하고 여기서는 고려하지 않는다.
#
## Wide Residual Networks
![image1](https://github.com/Doyosae/Speech_Enhancement/blob/master/image/05_1.png)
- Residual Network는 더 많은 컨볼루션 채널을 얻기 위해 위상적으로 모델의 깊이를 줄이고, 폭을 넓히는 형태이다.
- 이것은 Residual 연결의 장점을 살리고, 훈련을 가속화할 수 있다. 특히 gradient vanishing 문제가 많이 개선된다.
- 논문에서 제안하는 WRN의 구조의 기본 블럭은 두 가지 1차원 합성곱 레이어를 따라가는 것에서 시작한다.
이후에 BN와 ReLU같은 비선형 함수가 입, 출력 사이에 껴있다. 
- 이 네트워크에서 1D Conv 레이어는 1차원 시간 차원에서 작동한다. 이것은 이미지 프로세싱의 접근에서 기인한다.
- 스피치 프로세싱은 시각화된 스펙트럼에서 이루어지기도 한다. 이 경우에는 TF 도메인 상인데, 2D Conv 레이어를 사용한다.
- 이 둘의 차이는 필터가 이동하는 방식이다. 1D는 오직 시간축을 따라서 스트라이드하지만, 2D는 T, F 두 축을 따라 모두 스트라이드한다.
- 일반적으로 1차원 신호인 스피치는 슬라이딩 윈도우를 사용하여 처리한다. 그러므로 1차원 컨볼루션을 이러한 신호 핸들링에 고려할 만하다.
왜냐하면 1차원 컨볼루션이 매 시간마다 샘플 전체를 완전한 주파수 벡터로 변환할 수 있기 때문이다.
- 컨볼루션 레이어는 Time Delay Neural Networks의 특별한 케이스로 볼 수 있다. TDNN에서는 다음 샘플과 지나간 샘플을 네트워크 레이어의 입력으로써 concat되기 때문이다.
- 1차원 컨볼루션은 그것의 채널로 concat하여 서로 다른 정보들을 통합하기 쉽다. 이 방법으로, 신경망은 TF 도메인에서 제각기 다른 스피치 표현으로부터 얻은 특징 벡터들을 입력 받는다.
- 고속 푸리에 변환의 magnitude, Mel filterbank, cepstrum 등 각각이 손상된 신호 구조의 풍부한 묘사를 보여주도록 스피치 신호의 다양한 관점을 준다.
- analysis window size와 channel of the filterbank 사이의 불일치를 해결하기 위해, 특징 벡터들은 서로 세 개의 윈도우 사이즈를 가진다.
![image2](https://github.com/Doyosae/Speech_Enhancement/blob/master/image/05_3.png)
- 네트워크 구조는 일단 4개의 Wide Residual Block을 가진다. 이 중 첫 번째 WRB는 제일 앞의 단독으로 있는 컨볼루션 레이어의 출력과 입력을 처리한다.
- 뒤따라오는 WRB들의 구조는 아래의 모식도와 같다.
- 마지막 WRB 뒤에는 BN + PReLU를 사용하였다. 가장 마지막에는 1D ConvD 레이어가 있는데 이것의 활성함수는 ReLU를 사용하였다.
- 가장 끝의 1D 컨볼루션 레이어는 채널수를 1로 줄여서 각 신호에 대해 개선된 FFT를 획득한다. 
- WRB 각각은 출력을 얻어가면서 채널 수가 증가한다. 폭이 넓어지는 연산은 각 WRB, 첫 residual block의 첫 컨볼루션에서 수행된다.
- Residual Networks에서 합 연산을 수행하려면, 직선 경로에 있는 채널의 수와 컨볼루션 경로에 있는 채널 수가 같아야 한다.
- 그러므로 채널의 수가 증가할 때, 1차원 컨볼루션의 k = 1이 추가된다. 이것은 컨볼루션 경로의 채널의 수를 조절하기 위해 완전 연결층의 레이어로 해석할 수 있다.
![image3](https://github.com/Doyosae/Speech_Enhancement/blob/master/image/05_2.png)
